@book{AlexanderEA.1977,
  added-at = {2024-03-14T12:08:45.000+0100},
  address = {New York},
  author = {Alexander, Christopher and Ishikawa, Sara and Silverstein, Murray},
  biburl = {https://www.bibsonomy.org/bibtex/20cceee0bc9da6a045859c00cd0f5bf96/stefan.strecker},
  interhash = {0aa653751ca0087edb3cbf198f659bb5},
  intrahash = {0cceee0bc9da6a045859c00cd0f5bf96},
  keywords = {ArchitectureModeling SoftwareEngineering},
  publisher = {{Oxford University Press}},
  series = {Center for Environmental Structure Series},
  timestamp = {2024-03-14T12:08:45.000+0100},
  title = {A Pattern Language: Towns, buildings, construction},
  volume = 2,
  year = 1977
}

@book{gamma1994design,
  added-at = {2010-06-05T16:40:25.000+0200},
  asin = {0201633612},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John M.},
  biburl = {https://www.bibsonomy.org/bibtex/27e3f1154ab1fbce54752a46dba7f2217/pnk},
  description = {Amazon.com: Design Patterns: Elements of Reusable Object-Oriented Software (9780201633610): Erich Gamma, Richard Helm, Ralph Johnson, John M. Vlissides: Books},
  dewey = {005.12},
  ean = {9780201633610},
  edition = 1,
  interhash = {7fe32957be97afaf4ecb38b5490d23b4},
  intrahash = {7e3f1154ab1fbce54752a46dba7f2217},
  isbn = {0201633612},
  keywords = {DBIS Design Object-Oriented Patterns SS2010 Seminar Software},
  publisher = {Addison-Wesley Professional},
  timestamp = {2010-06-05T16:40:25.000+0200},
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  url = {http://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=ntt_at_ep_dpi_1},
  year = 1994
}

@INPROCEEDINGS{5291929,
  author={Sayar, Ahmet and Vural, Fatos T. Yarman},
  booktitle={2009 24th International Symposium on Computer and Information Sciences}, 
  title={Image annotation with semi-supervised clustering}, 
  year={2009},
  volume={},
  number={},
  pages={12-17},
  keywords={Clustering algorithms;Vocabulary;Concurrent computing;High performance computing;Image databases;Spatial databases;Visual databases;Information retrieval;Image retrieval;Image segmentation},
  doi={10.1109/ISCIS.2009.5291929}}

@INPROCEEDINGS{9689884,
  author={Haque, Md. Ashfaqul and Dristy, Israt Jahan and Tuhin, Mohammad Tariqul Islam and Sagar, Ali Hossain and Barek, Jayed Mohammad},
  booktitle={2021 24th International Conference on Computer and Information Technology (ICCIT)}, 
  title={Prediction on Intent Classification of Java and C\# Web queries using Semi-supervision}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  keywords={Java;Analytical models;Error analysis;Computational modeling;Training data;Search engines;Predictive models;Intent Classification;Semi-Supervised;Web queries;performance analysis;Java;C\#},
  doi={10.1109/ICCIT54785.2021.9689884}}

@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@INPROCEEDINGS{9463106,
  author={Mashhadi, Ehsan and Hemmati, Hadi},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)}, 
  title={Applying CodeBERT for Automated Program Repair of Java Simple Bugs}, 
  year={2021},
  volume={},
  number={},
  pages={505-509},
  keywords={Training;Java;Automation;Computer bugs;Computer architecture;Maintenance engineering;Data mining;Program repair;CodeBERT;Sequence to sequence learning;Transformers;Deep learning},
  doi={10.1109/MSR52588.2021.00063}}

@article{MAJIDZADEH2024103850,
title = {Multi-type requirements traceability prediction by code data augmentation and fine-tuning MS-CodeBERT},
journal = {Computer Standards \& Interfaces},
volume = {90},
pages = {103850},
year = {2024},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103850},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924000199},
author = {Ali Majidzadeh and Mehrdad Ashtiani and Morteza Zakeri-Nasrabadi},
keywords = {Requirement traceability, Language model, Data augmentation, Program transformation, Deep learning},
abstract = {Requirement traceability is a crucial quality factor that highly impacts the software evolution process and maintenance costs. Automated traceability links recovery techniques are required for a reliable and low-cost software development life cycle. Pre-trained language models have shown promising results on many natural language tasks. However, using such pre-trained models for requirement traceability needs large and quality traceability datasets and accurate fine-tuning mechanisms. This paper proposes code augmentation and fine-tuning techniques to prepare the MS-CodeBERT pre-trained language model for various types of requirements traceability prediction including documentation-to-method, issue-to-commit, and issue-to-method links. Three program transformation operations, namely, Rename Variable, Swap Operands, and Swap Statements are designed to generate new quality samples increasing the sample diversity of the traceability datasets. A 2-stage and 3-stage fine-tuning mechanism is proposed to fine-tune the language model for the three types of requirement traceability prediction on provided datasets. Experiments on 14 Java projects demonstrate a 6.2% to 8.5% improvement in the precision, 2.5% to 5.2% improvement in the recall, and 3.8% to 7.3% improvement in the F1 score of the traceability prediction models compared to the best results from the state-of-the-art methods.}
}

@INPROCEEDINGS{10298587,
  author={Liu, Jiaxing and Sha, Chaofeng and Peng, Xin},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models}, 
  year={2023},
  volume={},
  number={},
  pages={397-408},
  keywords={Training;Adaptation models;Codes;Costs;Computational modeling;Transfer learning;Memory management;pre-trained code models;fine-tuning;parameter-efficient;transfer learning},
  doi={10.1109/ASE56229.2023.00125}}

@INPROCEEDINGS{10298532,
  author={Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair}, 
  year={2023},
  volume={},
  number={},
  pages={1162-1174},
  keywords={Measurement;Java;Computer languages;Codes;Computer bugs;Maintenance engineering;Benchmark testing;Automated Program Repair;Large Language Models of Code;Neural Machine Translation;Fine-Tuning},
  doi={10.1109/ASE56229.2023.00181}}

@INPROCEEDINGS{4493325,
  author={Khomh, Foutse and Gueheneuc, Yann-Gael},
  booktitle={2008 12th European Conference on Software Maintenance and Reengineering}, 
  title={Do Design Patterns Impact Software Quality Positively?}, 
  year={2008},
  volume={},
  number={},
  pages={274-278},
  keywords={Software design;Software quality;Software maintenance;Concrete;Best practices;Impedance;Design engineering;Performance evaluation;Production facilities;Software systems},
  doi={10.1109/CSMR.2008.4493325}}

@INPROCEEDINGS{4301130,
  author={Abdul Jalil, Masita and Noah, Shahrul Azman Mohd},
  booktitle={2007 International Conference on Computational Science and its Applications (ICCSA 2007)}, 
  title={The Difficulties of Using Design Patterns among Novices: An Exploratory Study}, 
  year={2007},
  volume={},
  number={},
  pages={97-103},
  keywords={Application software;Computer science;Information science;Feedback;Computer industry;Software engineering;Software maintenance;Education;Computer applications;Software design},
  doi={10.1109/ICCSA.2007.32}}

@article{RIAZ201514,
title = {How have we evaluated software pattern application? A systematic mapping study of research design practices},
journal = {Information and Software Technology},
volume = {65},
pages = {14-38},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000774},
author = {Maria Riaz and Travis Breaux and Laurie Williams},
keywords = {Software pattern, Mapping study, Systematic review, Empirical evaluation, Empirical design},
}

@ARTICLE{988711,
  author={Prechelt, L. and Unger, B. and Tichy, W.F. and Brossler, P. and Votta, L.G.},
  journal={IEEE Transactions on Software Engineering}, 
  title={A controlled experiment in maintenance: comparing design patterns to simpler solutions}, 
  year={2001},
  volume={27},
  number={12},
  pages={1134-1144},
  keywords={Computer Society;Packaging;Software maintenance;Books;Delay effects;Runtime;Terminology;Testing;Solids;Guidelines},
  doi={10.1109/32.988711}}

@article{guo2022unixcoder,
  title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  author={Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
  journal={arXiv preprint arXiv:2203.03850},
  year={2022}
}

@article{ZHENG2023107194,
title = {An Abstract Syntax Tree based static fuzzing mutation for vulnerability evolution analysis},
journal = {Information and Software Technology},
volume = {158},
pages = {107194},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107194},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923000484},
author = {Wei Zheng and Peiran Deng and Kui Gui and Xiaoxue Wu},
keywords = {Static fuzzy mutation, Abstract Syntax Tree, Potential evolution paths, Concurrent Use After Free, Multi-threaded programs},
abstract = {Context:
Zero-day vulnerabilities are highly destructive and sudden. However, traditional static and dynamic testing methods cannot efficiently detect them.
Objective:
In this paper, a static fuzzy mutation method for program code is studied. This method can improve the efficiency of mutation sample generation according to the vulnerability evolution law, thus promoting the development of zero-day vulnerability detection methods based on deep learning techniques.
Method:
A static fuzzy mutation method based on the Abstract Syntax Tree (AST) is proposed. Under the guidance of software vulnerability evolution law, potential evolution paths that threaten program security are detected, and mutation samples containing vulnerabilities are generated at the syntax tree level based on the paths. To verify the effectiveness of static fuzzy mutation based on ASTs, this paper starts with Concurrent Use After Free (CUAF) homologous vulnerability. It uses multi-threaded programs to perform vulnerability feature statement insertion processing to infer the optimal mutation operator execution sequence corresponding to CUAF vulnerabilities triggered by data competition. The Linux kernel code is used to verify whether it can effectively reduce the number of invalid mutation samples.
Results:
In this paper, we filter the code fragments in the Linux kernel public code containing CUAF vulnerability fix commits and perform static fuzzy mutation on the fix versions of the vulnerabilities to reproduce the vulnerabilities of this type triggered by these code fragments on the timeline. We compare the process with the execution of the random mutation operator in traditional detection methods horizontally and improve the efficiency by 42.4% on average.
Conclusion:
The static fuzzy mutation based on the AST is effective in stages. When this method is explored in more vulnerability-type evolution laws, it is expected to promote the development of the zero-day vulnerability active detection technology framework.}
}